{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sofmax classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Create model\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# construct model\n",
    "activation = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(activation), reduction_indices=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train_data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting train_data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting train_data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting train_data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"train_data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 2.045250847\n",
      "Epoch: 0002 cost= 1.654342352\n",
      "Epoch: 0003 cost= 1.394379731\n",
      "Epoch: 0004 cost= 1.217121436\n",
      "Epoch: 0005 cost= 1.091352293\n",
      "Epoch: 0006 cost= 0.998395672\n",
      "Epoch: 0007 cost= 0.927083710\n",
      "Epoch: 0008 cost= 0.870712692\n",
      "Epoch: 0009 cost= 0.824963170\n",
      "Epoch: 0010 cost= 0.787089726\n",
      "Epoch: 0011 cost= 0.755134265\n",
      "Epoch: 0012 cost= 0.727799890\n",
      "Epoch: 0013 cost= 0.704095437\n",
      "Epoch: 0014 cost= 0.683324346\n",
      "Epoch: 0015 cost= 0.664962629\n",
      "Epoch: 0016 cost= 0.648580983\n",
      "Epoch: 0017 cost= 0.633867374\n",
      "Epoch: 0018 cost= 0.620560943\n",
      "Epoch: 0019 cost= 0.608461673\n",
      "Epoch: 0020 cost= 0.597412210\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "for epoch in range(training_epochs) :\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch) :\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8737\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(y, 1))\n",
    "# Calcuate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (\"Accuracy: \", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}, session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets (NN) for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = tf.placeholder(\"float\", [None, 784])\n",
    "# Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "# Store layers weight & bias\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(x, W1), B1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), B2)) # Hidden layer with ReLU activation\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), B3) # No need to use softmax here\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 143.978414551\n",
      "Epoch: 0002 cost= 37.283901539\n",
      "Epoch: 0003 cost= 23.363045398\n",
      "Epoch: 0004 cost= 16.059688560\n",
      "Epoch: 0005 cost= 11.197932456\n",
      "Epoch: 0006 cost= 8.162884131\n",
      "Epoch: 0007 cost= 5.807968107\n",
      "Epoch: 0008 cost= 4.329392209\n",
      "Epoch: 0009 cost= 3.057594937\n",
      "Epoch: 0010 cost= 2.122608865\n",
      "Epoch: 0011 cost= 1.480753655\n",
      "Epoch: 0012 cost= 0.971804551\n",
      "Epoch: 0013 cost= 0.718129140\n",
      "Epoch: 0014 cost= 0.527683856\n",
      "Epoch: 0015 cost= 0.385215690\n",
      "Epoch: 0016 cost= 0.325198265\n",
      "Epoch: 0017 cost= 0.192141381\n",
      "Epoch: 0018 cost= 0.164326945\n",
      "Epoch: 0019 cost= 0.129780885\n",
      "Epoch: 0020 cost= 0.129363988\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "for epoch in range(training_epochs) :\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch) :\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9492\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "# Calcuate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (\"Accuracy: \", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}, session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "  \"\"\"Set the parameter initialization using the method described.\n",
    "  This method is designed to keep the scale of the gradients roughly the same\n",
    "  in all layers.\n",
    "  Xavier Glorot and Yoshua Bengio (2010):\n",
    "           Understanding the difficulty of training deep feedforward neural\n",
    "           networks. International conference on artificial intelligence and\n",
    "           statistics.\n",
    "  Args:\n",
    "    n_inputs: The number of input nodes into each output.\n",
    "    n_outputs: The number of output nodes for each input.\n",
    "    uniform: If true use a uniform distribution, otherwise use a normal.\n",
    "  Returns:\n",
    "    An initializer.\n",
    "  \"\"\"\n",
    "  if uniform:\n",
    "    # 6 was used in the paper.\n",
    "    init_range = math.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    return tf.random_uniform_initializer(-init_range, init_range)\n",
    "  else:\n",
    "    # 3 gives us approximately the same limits as above since this repicks\n",
    "    # values greater than 2 standard deviations from the mean.\n",
    "    stddev = math.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "    return tf.truncated_normal_initializer(stddev=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=xavier_init(784, 256))\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=xavier_init(256, 256))\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10], initializer=xavier_init(256, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(x, W1), B1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), B2)) # Hidden layer with ReLU activation\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), B3) # No need to use softmax here\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.291964835\n",
      "Epoch: 0002 cost= 0.100902805\n",
      "Epoch: 0003 cost= 0.063329751\n",
      "Epoch: 0004 cost= 0.042546163\n",
      "Epoch: 0005 cost= 0.032396157\n",
      "Epoch: 0006 cost= 0.021808150\n",
      "Epoch: 0007 cost= 0.016180384\n",
      "Epoch: 0008 cost= 0.011601979\n",
      "Epoch: 0009 cost= 0.008895923\n",
      "Epoch: 0010 cost= 0.008439940\n",
      "Epoch: 0011 cost= 0.006495009\n",
      "Epoch: 0012 cost= 0.005064241\n",
      "Epoch: 0013 cost= 0.004133837\n",
      "Epoch: 0014 cost= 0.004575897\n",
      "Epoch: 0015 cost= 0.003599783\n",
      "Epoch: 0016 cost= 0.003051421\n",
      "Epoch: 0017 cost= 0.003456137\n",
      "Epoch: 0018 cost= 0.002207229\n",
      "Epoch: 0019 cost= 0.003172260\n",
      "Epoch: 0020 cost= 0.001303437\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "for epoch in range(training_epochs) :\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch) :\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9784\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "# Calcuate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (\"Accuracy: \", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}, session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More deep & dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "W1 = tf.get_variable(\"W9\", shape=[784, 512], initializer=xavier_init(784, 256))\n",
    "W2 = tf.get_variable(\"W10\", shape=[512, 256], initializer=xavier_init(256, 256))\n",
    "W3 = tf.get_variable(\"W11\", shape=[256, 128], initializer=xavier_init(256, 256))\n",
    "W4 = tf.get_variable(\"W12\", shape=[128, 64], initializer=xavier_init(256, 256))\n",
    "W5 = tf.get_variable(\"W13\", shape=[64, 10], initializer=xavier_init(256, 10))\n",
    "\n",
    "# W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "# W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "# W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "# W4 = tf.Variable(tf.random_normal([256, 256]))\n",
    "# W5 = tf.Variable(tf.random_normal([256, 10]))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([512]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([128]))\n",
    "B4 = tf.Variable(tf.random_normal([64]))\n",
    "B5 = tf.Variable(tf.random_normal([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct model\n",
    "dropout_rate = tf.placeholder(\"float\")\n",
    "_L1 = tf.nn.relu(tf.add(tf.matmul(x, W1), B1)) # Hidden layer with ReLU activation\n",
    "L1 = tf.nn.dropout(_L1, dropout_rate)\n",
    "_L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), B2)) # Hidden layer with ReLU activation\n",
    "L2 = tf.nn.dropout(_L2, dropout_rate)\n",
    "_L3 = tf.nn.relu(tf.add(tf.matmul(L2, W3), B3)) # Hidden layer with ReLU activation\n",
    "L3 = tf.nn.dropout(_L3, dropout_rate)\n",
    "_L4 = tf.nn.relu(tf.add(tf.matmul(L3, W4), B4)) # Hidden layer with ReLU activation\n",
    "L4 = tf.nn.dropout(_L4, dropout_rate)\n",
    "\n",
    "hypothesis = tf.add(tf.matmul(L4, W5), B5) # No need to use softmax here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.545545795\n",
      "Epoch: 0002 cost= 0.193876461\n",
      "Epoch: 0003 cost= 0.145715331\n",
      "Epoch: 0004 cost= 0.113307944\n",
      "Epoch: 0005 cost= 0.099963960\n",
      "Epoch: 0006 cost= 0.086533619\n",
      "Epoch: 0007 cost= 0.076751394\n",
      "Epoch: 0008 cost= 0.068687864\n",
      "Epoch: 0009 cost= 0.062506411\n",
      "Epoch: 0010 cost= 0.057912409\n",
      "Epoch: 0011 cost= 0.051505577\n",
      "Epoch: 0012 cost= 0.049625208\n",
      "Epoch: 0013 cost= 0.047798115\n",
      "Epoch: 0014 cost= 0.043129074\n",
      "Epoch: 0015 cost= 0.041926224\n",
      "Epoch: 0016 cost= 0.038653051\n",
      "Epoch: 0017 cost= 0.039361055\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs) :\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch) :\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, dropout_rate: 0.7})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, dropout_rate: 0.7})/total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "# Calcuate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (\"Accuracy: \", accuracy.eval({x: mnist.test.images, y: mnist.test.labels, dropout_rate: 1.0}, session=sess))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
