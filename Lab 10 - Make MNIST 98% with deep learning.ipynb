{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sofmax classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 101\n",
    "batch_size = 100\n",
    "display_step = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Create model\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# construct model\n",
    "activation = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(activation), reduction_indices=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train_data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting train_data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting train_data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting train_data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"train_data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 2.045250847\n",
      "Epoch: 0021 cost= 0.587260515\n",
      "Epoch: 0041 cost= 0.477556967\n",
      "Epoch: 0061 cost= 0.431832330\n",
      "Epoch: 0081 cost= 0.405301124\n",
      "Epoch: 0101 cost= 0.387421326\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "for epoch in range(training_epochs) :\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch) :\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9034\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(y, 1))\n",
    "# Calcuate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (\"Accuracy: \", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}, session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets (NN) for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = tf.placeholder(\"float\", [None, 784])\n",
    "# Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "# Store layers weight & bias\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(x, W1), B1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), B2)) # Hidden layer with ReLU activation\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), B3) # No need to use softmax here\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 166.966616943\n",
      "Epoch: 0021 cost= 0.183849186\n",
      "Epoch: 0041 cost= 0.054312433\n",
      "Epoch: 0061 cost= 0.028181199\n",
      "Epoch: 0081 cost= 0.022873562\n",
      "Epoch: 0101 cost= 0.004413743\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "for epoch in range(training_epochs) :\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch) :\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9693\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "# Calcuate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (\"Accuracy: \", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}, session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "  \"\"\"Set the parameter initialization using the method described.\n",
    "  This method is designed to keep the scale of the gradients roughly the same\n",
    "  in all layers.\n",
    "  Xavier Glorot and Yoshua Bengio (2010):\n",
    "           Understanding the difficulty of training deep feedforward neural\n",
    "           networks. International conference on artificial intelligence and\n",
    "           statistics.\n",
    "  Args:\n",
    "    n_inputs: The number of input nodes into each output.\n",
    "    n_outputs: The number of output nodes for each input.\n",
    "    uniform: If true use a uniform distribution, otherwise use a normal.\n",
    "  Returns:\n",
    "    An initializer.\n",
    "  \"\"\"\n",
    "  if uniform:\n",
    "    # 6 was used in the paper.\n",
    "    init_range = math.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    return tf.random_uniform_initializer(-init_range, init_range)\n",
    "  else:\n",
    "    # 3 gives us approximately the same limits as above since this repicks\n",
    "    # values greater than 2 standard deviations from the mean.\n",
    "    stddev = math.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "    return tf.truncated_normal_initializer(stddev=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256], initializer=xavier_init(784, 256))\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256], initializer=xavier_init(256, 256))\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10], initializer=xavier_init(256, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B1 = tf.Variable(tf.random_normal([256]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# Construct model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(x, W1), B1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), B2)) # Hidden layer with ReLU activation\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), B3) # No need to use softmax here\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.308614404\n",
      "Epoch: 0021 cost= 0.001740475\n",
      "Epoch: 0041 cost= 0.001272665\n",
      "Epoch: 0061 cost= 0.000479724\n",
      "Epoch: 0081 cost= 0.000000357\n",
      "Epoch: 0101 cost= 0.000000002\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "for epoch in range(training_epochs) :\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch) :\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9844\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "# Calcuate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (\"Accuracy: \", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}, session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More deep & dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "W1 = tf.get_variable(\"W9\", shape=[784, 512], initializer=xavier_init(784, 512))\n",
    "W2 = tf.get_variable(\"W10\", shape=[512, 256], initializer=xavier_init(512, 256))\n",
    "W3 = tf.get_variable(\"W11\", shape=[256, 128], initializer=xavier_init(256, 128))\n",
    "W4 = tf.get_variable(\"W12\", shape=[128, 64], initializer=xavier_init(128, 64))\n",
    "W5 = tf.get_variable(\"W13\", shape=[64, 10], initializer=xavier_init(64, 10))\n",
    "\n",
    "# W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "# W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "# W3 = tf.Variable(tf.random_normal([256, 256]))\n",
    "# W4 = tf.Variable(tf.random_normal([256, 256]))\n",
    "# W5 = tf.Variable(tf.random_normal([256, 10]))\n",
    "\n",
    "B1 = tf.Variable(tf.random_normal([512]))\n",
    "B2 = tf.Variable(tf.random_normal([256]))\n",
    "B3 = tf.Variable(tf.random_normal([128]))\n",
    "B4 = tf.Variable(tf.random_normal([64]))\n",
    "B5 = tf.Variable(tf.random_normal([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct model\n",
    "dropout_rate = tf.placeholder(\"float\")\n",
    "_L1 = tf.nn.relu(tf.add(tf.matmul(x, W1), B1)) # Hidden layer with ReLU activation\n",
    "L1 = tf.nn.dropout(_L1, dropout_rate)\n",
    "_L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), B2)) # Hidden layer with ReLU activation\n",
    "L2 = tf.nn.dropout(_L2, dropout_rate)\n",
    "_L3 = tf.nn.relu(tf.add(tf.matmul(L2, W3), B3)) # Hidden layer with ReLU activation\n",
    "L3 = tf.nn.dropout(_L3, dropout_rate)\n",
    "_L4 = tf.nn.relu(tf.add(tf.matmul(L3, W4), B4)) # Hidden layer with ReLU activation\n",
    "L4 = tf.nn.dropout(_L4, dropout_rate)\n",
    "\n",
    "hypothesis = tf.add(tf.matmul(L4, W5), B5) # No need to use softmax here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, y)) # Softmax loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Adam Optimizer\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.630215467\n",
      "Epoch: 0021 cost= 0.035704602\n",
      "Epoch: 0041 cost= 0.022299135\n",
      "Epoch: 0061 cost= 0.017929360\n",
      "Epoch: 0081 cost= 0.014924662\n",
      "Epoch: 0101 cost= 0.013403191\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs) :\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch) :\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, dropout_rate: 0.7})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, dropout_rate: 0.7})/total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9848\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y, 1))\n",
    "# Calcuate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (\"Accuracy: \", accuracy.eval({x: mnist.test.images, y: mnist.test.labels, dropout_rate: 1.0}, session=sess))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
