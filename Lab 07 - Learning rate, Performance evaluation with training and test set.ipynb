{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('train_data/lab06_train.txt', unpack=True, dtype='float32')\n",
    "x_data = np.transpose(xy[0:3])\n",
    "y_data = np.transpose(xy[3:])\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\", [None, 3]) # x1, x2 and 1 (for bias)\n",
    "Y = tf.placeholder(\"float\", [None, 3]) # A, B, C => 3 classes\n",
    "\n",
    "# Set modle weights\n",
    "W = tf.Variable(tf.zeros([3, 3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W)) # Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate를 큰 값으로 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy\n",
    "learning_rate = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nan [[-0.83333319  0.4166666   0.41666645]\n",
      " [ 1.66666687  2.91666746 -4.58333397]\n",
      " [ 1.66666627  4.16666698 -5.83333397]]\n",
      "200 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "400 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "600 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "800 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1000 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1200 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1400 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1600 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1800 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "2000 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n"
     ]
    }
   ],
   "source": [
    "# Corss entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\n",
    "\n",
    "#Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 200 == 0:\n",
    "        print (step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate이 너무 커서 overshooting이 되어서 학습이 전혀 일어나지 않않다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate를 작은 값으로 주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.09852 [[ -8.33333161e-06   4.16666580e-06   4.16666444e-06]\n",
      " [  1.66666687e-05   2.91666747e-05  -4.58333379e-05]\n",
      " [  1.66666614e-05   4.16666662e-05  -5.83333349e-05]]\n",
      "200 1.08514 [[-0.00175495  0.00064726  0.00110769]\n",
      " [ 0.00293629  0.00489164 -0.00782793]\n",
      " [ 0.00294097  0.00740635 -0.01034732]]\n",
      "400 1.07722 [[-0.0036236   0.00098153  0.00264207]\n",
      " [ 0.00522766  0.00818141 -0.01340907]\n",
      " [ 0.00524379  0.01319837 -0.01844216]]\n",
      "600 1.07226 [[-0.00557707  0.00108985  0.00448722]\n",
      " [ 0.00708463  0.01032533 -0.01740997]\n",
      " [ 0.0071169   0.01783946 -0.02495635]]\n",
      "800 1.06895 [[-0.00759301  0.00103238  0.00656063]\n",
      " [ 0.00862161  0.01163302 -0.02025463]\n",
      " [ 0.00867387  0.02163565 -0.03030949]]\n",
      "1000 1.06658 [[-0.00965698  0.00085276  0.00880422]\n",
      " [ 0.00991114  0.01232841 -0.02223956]\n",
      " [ 0.00998697  0.02480848 -0.03479542]]\n",
      "1200 1.06475 [[-0.01175893  0.00058266  0.01117626]\n",
      " [ 0.01100301  0.01257371 -0.02357675]\n",
      " [ 0.01110593  0.02751854 -0.03862444]]\n",
      "1400 1.06325 [[-0.01389133  0.00024516  0.01364617]\n",
      " [ 0.01193393  0.0124868  -0.02442075]\n",
      " [ 0.01206745  0.0298826  -0.04195001]]\n",
      "1600 1.06195 [[-0.01604833 -0.00014285  0.01619117]\n",
      " [ 0.01273244  0.01215373 -0.02488619]\n",
      " [ 0.01290009  0.031986   -0.04488603]]\n",
      "1800 1.06077 [[-0.01822517 -0.00056892  0.01879408]\n",
      " [ 0.01342153  0.01163764 -0.02505921]\n",
      " [ 0.01362685  0.0338914  -0.04751818]]\n",
      "2000 1.05967 [[-0.02041796 -0.00102386  0.02144181]\n",
      " [ 0.01402011  0.01098517 -0.02500531]\n",
      " [ 0.01426658  0.03564513 -0.04991166]]\n"
     ]
    }
   ],
   "source": [
    "# Corss entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\n",
    "\n",
    "#Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 200 == 0:\n",
    "        print (step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate를 너무 작은 값으로 주어서 학습이 거의 일어나지 않았다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0678 [[-0.00833333  0.00416667  0.00416666]\n",
      " [ 0.01666667  0.02916667 -0.04583334]\n",
      " [ 0.01666666  0.04166667 -0.05833334]]\n",
      "200 0.699681 [[-1.5737735  -0.36410642  1.93788099]\n",
      " [ 0.0967759  -0.09235884 -0.00441682]\n",
      " [ 0.24915566  0.23823613 -0.48739177]]\n",
      "400 0.594579 [[-2.54309034 -0.43173078  2.97482228]\n",
      " [ 0.13064831 -0.04841127 -0.08223619]\n",
      " [ 0.40734777  0.22857714 -0.63592446]]\n",
      "600 0.535829 [[-3.31521416 -0.39115471  3.70637012]\n",
      " [ 0.13982886 -0.02464214 -0.11518568]\n",
      " [ 0.54843497  0.21297167 -0.76140594]]\n",
      "800 0.494312 [[-3.98393679 -0.31143472  4.29537106]\n",
      " [ 0.1417881  -0.00935038 -0.13243635]\n",
      " [ 0.6748786   0.1950776  -0.86995488]]\n",
      "1000 0.461916 [[ -4.58334923e+00  -2.18527570e-01   4.80187750e+00]\n",
      " [  1.41456679e-01   1.54198625e-03  -1.42996773e-01]\n",
      " [  7.89354861e-01   1.77009344e-01  -9.66362596e-01]]\n",
      "1200 0.435343 [[-5.13021469 -0.12343737  5.25365353]\n",
      " [ 0.14044461  0.00979479 -0.15023731]\n",
      " [ 0.89399701  0.15975396 -1.05374885]]\n",
      "1400 0.412891 [[-5.63466215 -0.03106253  5.66572714]\n",
      " [ 0.1393245   0.01630212 -0.15562418]\n",
      " [ 0.99044132  0.14372151 -1.13416028]]\n",
      "1600 0.393535 [[-6.10366583  0.05645988  6.04720736]\n",
      " [ 0.13829574  0.02157287 -0.15986556]\n",
      " [ 1.07996321  0.12904176 -1.20900333]]\n",
      "1800 0.376595 [[-6.54242659  0.13831529  6.40411282]\n",
      " [ 0.13741526  0.02592302 -0.16333508]\n",
      " [ 1.16357517  0.11571171 -1.27928567]]\n",
      "2000 0.361591 [[-6.95501471  0.21433842  6.7406764 ]\n",
      " [ 0.13668649  0.02956286 -0.16624542]\n",
      " [ 1.2420913   0.1036661  -1.3457557 ]]\n"
     ]
    }
   ],
   "source": [
    "# Corss entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\n",
    "\n",
    "#Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 200 == 0:\n",
    "        print (step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어느정도 학습이 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset\n",
    "\n",
    "데이터를 저장한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Functions for downloading and reading MNIST data.\"\"\"\n",
    "from __future__ import print_function\n",
    "import gzip\n",
    "import os\n",
    "from urllib import request\n",
    "\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "\n",
    "def maybe_download(filename, work_directory):\n",
    "  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
    "  if not os.path.exists(work_directory):\n",
    "    os.mkdir(work_directory)\n",
    "  filepath = os.path.join(work_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    filepath, _ = request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  return filepath\n",
    "\n",
    "def _read32(bytestream):\n",
    "  dt = np.dtype(np.uint32).newbyteorder('>')\n",
    "  return np.frombuffer(bytestream.read(4), dtype=dt)\n",
    "\n",
    "def extract_images(filename):\n",
    "  \"\"\"Extract the images into a 4D uint8 np.array [index, y, x, depth].\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    magic = _read32(bytestream)\n",
    "    if magic != 2051:\n",
    "      raise ValueError(\n",
    "          'Invalid magic number %d in MNIST image file: %s' %\n",
    "          (magic, filename))\n",
    "    num_images = _read32(bytestream)\n",
    "    rows = _read32(bytestream)\n",
    "    cols = _read32(bytestream)\n",
    "    buf = bytestream.read(rows * cols * num_images)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8)\n",
    "    data = data.reshape(num_images, rows, cols, 1)\n",
    "    return data\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "  num_labels = labels_dense.shape[0]\n",
    "  index_offset = np.arange(num_labels) * num_classes\n",
    "  labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "  return labels_one_hot\n",
    "\n",
    "def extract_labels(filename, one_hot=False):\n",
    "  \"\"\"Extract the labels into a 1D uint8 np.array [index].\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    magic = _read32(bytestream)\n",
    "    if magic != 2049:\n",
    "      raise ValueError(\n",
    "          'Invalid magic number %d in MNIST label file: %s' %\n",
    "          (magic, filename))\n",
    "    num_items = _read32(bytestream)\n",
    "    buf = bytestream.read(num_items)\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8)\n",
    "    if one_hot:\n",
    "      return dense_to_one_hot(labels)\n",
    "    return labels\n",
    "\n",
    "class DataSet(object):\n",
    "  def __init__(self, images, labels, fake_data=False):\n",
    "    if fake_data:\n",
    "      self._num_examples = 10000\n",
    "    else:\n",
    "      assert images.shape[0] == labels.shape[0], (\n",
    "          \"images.shape: %s labels.shape: %s\" % (images.shape,\n",
    "                                                 labels.shape))\n",
    "      self._num_examples = images.shape[0]\n",
    "      # Convert shape from [num examples, rows, columns, depth]\n",
    "      # to [num examples, rows*columns] (assuming depth == 1)\n",
    "      assert images.shape[3] == 1\n",
    "      images = images.reshape(images.shape[0],\n",
    "                              images.shape[1] * images.shape[2])\n",
    "      # Convert from [0, 255] -> [0.0, 1.0].\n",
    "      images = images.astype(np.float32)\n",
    "      images = np.multiply(images, 1.0 / 255.0)\n",
    "    self._images = images\n",
    "    self._labels = labels\n",
    "    self._epochs_completed = 0\n",
    "    self._index_in_epoch = 0\n",
    "    \n",
    "  @property\n",
    "  def images(self):\n",
    "    return self._images\n",
    "\n",
    "  @property\n",
    "  def labels(self):\n",
    "    return self._labels\n",
    "\n",
    "  @property\n",
    "  def num_examples(self):\n",
    "    return self._num_examples\n",
    "\n",
    "  @property\n",
    "  def epochs_completed(self):\n",
    "    return self._epochs_completed\n",
    "\n",
    "  def next_batch(self, batch_size, fake_data=False):\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "    if fake_data:\n",
    "      fake_image = [1.0 for _ in xrange(784)]\n",
    "      fake_label = 0\n",
    "      return [fake_image for _ in xrange(batch_size)], [\n",
    "          fake_label for _ in xrange(batch_size)]\n",
    "    start = self._index_in_epoch\n",
    "    self._index_in_epoch += batch_size\n",
    "    if self._index_in_epoch > self._num_examples:\n",
    "      # Finished epoch\n",
    "      self._epochs_completed += 1\n",
    "      # Shuffle the data\n",
    "      perm = np.arange(self._num_examples)\n",
    "      np.random.shuffle(perm)\n",
    "      self._images = self._images[perm]\n",
    "      self._labels = self._labels[perm]\n",
    "      # Start next epoch\n",
    "      start = 0\n",
    "      self._index_in_epoch = batch_size\n",
    "      assert batch_size <= self._num_examples\n",
    "    end = self._index_in_epoch\n",
    "    return self._images[start:end], self._labels[start:end]\n",
    "\n",
    "def read_data_sets(train_dir, fake_data=False, one_hot=False):\n",
    "  class DataSets(object):\n",
    "    pass\n",
    "  data_sets = DataSets()\n",
    "  if fake_data:\n",
    "    data_sets.train = DataSet([], [], fake_data=True)\n",
    "    data_sets.validation = DataSet([], [], fake_data=True)\n",
    "    data_sets.test = DataSet([], [], fake_data=True)\n",
    "    return data_sets\n",
    "  TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "  TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "  TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "  TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "  VALIDATION_SIZE = 5000\n",
    "  local_file = maybe_download(TRAIN_IMAGES, train_dir)\n",
    "  train_images = extract_images(local_file)\n",
    "  local_file = maybe_download(TRAIN_LABELS, train_dir)\n",
    "  train_labels = extract_labels(local_file, one_hot=one_hot)\n",
    "  local_file = maybe_download(TEST_IMAGES, train_dir)\n",
    "  test_images = extract_images(local_file)\n",
    "  local_file = maybe_download(TEST_LABELS, train_dir)\n",
    "  test_labels = extract_labels(local_file, one_hot=one_hot)\n",
    "  validation_images = train_images[:VALIDATION_SIZE]\n",
    "  validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "  train_images = train_images[VALIDATION_SIZE:]\n",
    "  train_labels = train_labels[VALIDATION_SIZE:]\n",
    "  data_sets.train = DataSet(train_images, train_labels)\n",
    "  data_sets.validation = DataSet(validation_images, validation_labels)\n",
    "  data_sets.test = DataSet(test_images, test_labels)\n",
    "  return data_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data and set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activiation (hypothesis) and cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# construct model\n",
    "activation = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(activation), reduction_indices=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training batch\n",
    "\n",
    "training data set이 많기 때문에 잘라서 학습 시킨다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train_data/MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.4/gzip.py:372: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n",
      "  chunk = self.extrabuf[offset: offset + size]\n",
      "/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py:38: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train_data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting train_data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting train_data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "laerning_rate = 0.01\n",
    "training_epochs = 5\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mnist = read_data_sets(\"train_data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.521184075\n",
      "Epoch: 0002 cost= 0.352781631\n",
      "Epoch: 0003 cost= 0.324449328\n",
      "Epoch: 0004 cost= 0.309543791\n",
      "Epoch: 0005 cost= 0.299857430\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Training cycle\n",
    "for epoch in range(training_epochs) :\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch) :\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "print (\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Predict & Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [3]\n",
      "Predication:  [3]\n"
     ]
    }
   ],
   "source": [
    "# Get one and predict\n",
    "from random import randint\n",
    "\n",
    "r = randint(0, mnist.test.num_examples - 1)\n",
    "print (\"Label: \", sess.run(tf.argmax(activation, 1), {x: mnist.test.images[r:r+1]}))\n",
    "print (\"Predication: \", sess.run(tf.argmax(activation, 1), {x: mnist.test.images[r:r+1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFfCAYAAACfj30KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIkNYkhL9H5/Gzqwqp6PQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f940a53e4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the img\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r+1].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "test data set을 다 돌려서 정확도가 얼마인지 본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9166\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "correct_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(y, 1))\n",
    "# Calcuate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print (\"Accuracy: \", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}, session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy.eval이 sess.run과 같은 역할을 함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
