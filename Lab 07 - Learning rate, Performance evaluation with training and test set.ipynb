{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('train_data/lab06_train.txt', unpack=True, dtype='float32')\n",
    "x_data = np.transpose(xy[0:3])\n",
    "y_data = np.transpose(xy[3:])\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\", [None, 3]) # x1, x2 and 1 (for bias)\n",
    "Y = tf.placeholder(\"float\", [None, 3]) # A, B, C => 3 classes\n",
    "\n",
    "# Set modle weights\n",
    "W = tf.Variable(tf.zeros([3, 3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W)) # Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate를 큰 값으로 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy\n",
    "learning_rate = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nan [[-0.83333319  0.4166666   0.41666645]\n",
      " [ 1.66666687  2.91666746 -4.58333397]\n",
      " [ 1.66666627  4.16666698 -5.83333397]]\n",
      "200 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "400 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "600 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "800 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1000 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1200 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1400 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1600 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1800 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "2000 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n"
     ]
    }
   ],
   "source": [
    "# Corss entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\n",
    "\n",
    "#Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 200 == 0:\n",
    "        print (step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate이 너무 커서 overshooting이 되어서 학습이 전혀 일어나지 않않다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate를 작은 값으로 주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.09774 [[ -8.33333252e-05   4.16666626e-05   4.16666480e-05]\n",
      " [  1.66666694e-04   2.91666773e-04  -4.58333408e-04]\n",
      " [  1.66666636e-04   4.16666706e-04  -5.83333429e-04]]\n",
      "200 1.05962 [[-0.02051384 -0.00103983  0.02155367]\n",
      " [ 0.01406438  0.01097753 -0.02504191]\n",
      " [ 0.01431208  0.03574873 -0.05006079]]\n",
      "400 1.04985 [[-0.04282598 -0.00625899  0.04908497]\n",
      " [ 0.01747187  0.00156368 -0.01903554]\n",
      " [ 0.01831204  0.04954104 -0.06785304]]\n",
      "600 1.0407 [[-0.06517859 -0.01176361  0.07694222]\n",
      " [ 0.01943521 -0.00848972 -0.01094548]\n",
      " [ 0.0211558   0.06118308 -0.0823388 ]]\n",
      "800 1.03194 [[-0.08734013 -0.01729389  0.10463405]\n",
      " [ 0.0211172  -0.01796171 -0.00315548]\n",
      " [ 0.02396628  0.07198346 -0.09594974]]\n",
      "1000 1.02354 [[-0.10928574 -0.02282181  0.13210757]\n",
      " [ 0.02266254 -0.02678034  0.00411783]\n",
      " [ 0.02685853  0.08210357 -0.10896212]]\n",
      "1200 1.01547 [[-0.13101467 -0.02834092  0.15935561]\n",
      " [ 0.02409703 -0.03497621  0.01087923]\n",
      " [ 0.029832    0.091598   -0.12143001]]\n",
      "1400 1.0077 [[-0.15252906 -0.03384715  0.18637618]\n",
      " [ 0.02543302 -0.04258838  0.01715542]\n",
      " [ 0.03287466  0.10050805 -0.13338271]]\n",
      "1600 1.00021 [[-0.17383142 -0.03933692  0.21316831]\n",
      " [ 0.02668081 -0.04965464  0.0229739 ]\n",
      " [ 0.03597427  0.10887136 -0.14484565]]\n",
      "1800 0.992979 [[-0.19492456 -0.04480688  0.23973146]\n",
      " [ 0.02784993 -0.05621058  0.02836074]\n",
      " [ 0.03911954  0.11672314 -0.15584265]]\n",
      "2000 0.985988 [[-0.21581143 -0.05025396  0.26606542]\n",
      " [ 0.02894915 -0.06228962  0.03334056]\n",
      " [ 0.04230019  0.12409624 -0.16639642]]\n"
     ]
    }
   ],
   "source": [
    "# Corss entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\n",
    "\n",
    "#Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 200 == 0:\n",
    "        print (step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate를 너무 작은 값으로 주어서 학습이 거의 일어나지 않았다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0678 [[-0.00833333  0.00416667  0.00416666]\n",
      " [ 0.01666667  0.02916667 -0.04583334]\n",
      " [ 0.01666666  0.04166667 -0.05833334]]\n",
      "200 0.699681 [[-1.5737735  -0.36410642  1.93788099]\n",
      " [ 0.0967759  -0.09235884 -0.00441682]\n",
      " [ 0.24915566  0.23823613 -0.48739177]]\n",
      "400 0.594579 [[-2.54309034 -0.43173078  2.97482228]\n",
      " [ 0.13064831 -0.04841127 -0.08223619]\n",
      " [ 0.40734777  0.22857714 -0.63592446]]\n",
      "600 0.535829 [[-3.31521416 -0.39115471  3.70637012]\n",
      " [ 0.13982886 -0.02464214 -0.11518568]\n",
      " [ 0.54843497  0.21297167 -0.76140594]]\n",
      "800 0.494312 [[-3.98393679 -0.31143472  4.29537106]\n",
      " [ 0.1417881  -0.00935038 -0.13243635]\n",
      " [ 0.6748786   0.1950776  -0.86995488]]\n",
      "1000 0.461916 [[ -4.58334923e+00  -2.18527570e-01   4.80187750e+00]\n",
      " [  1.41456679e-01   1.54198625e-03  -1.42996773e-01]\n",
      " [  7.89354861e-01   1.77009344e-01  -9.66362596e-01]]\n",
      "1200 0.435343 [[-5.13021469 -0.12343737  5.25365353]\n",
      " [ 0.14044461  0.00979479 -0.15023731]\n",
      " [ 0.89399701  0.15975396 -1.05374885]]\n",
      "1400 0.412891 [[-5.63466215 -0.03106253  5.66572714]\n",
      " [ 0.1393245   0.01630212 -0.15562418]\n",
      " [ 0.99044132  0.14372151 -1.13416028]]\n",
      "1600 0.393535 [[-6.10366583  0.05645988  6.04720736]\n",
      " [ 0.13829574  0.02157287 -0.15986556]\n",
      " [ 1.07996321  0.12904176 -1.20900333]]\n",
      "1800 0.376595 [[-6.54242659  0.13831529  6.40411282]\n",
      " [ 0.13741526  0.02592302 -0.16333508]\n",
      " [ 1.16357517  0.11571171 -1.27928567]]\n",
      "2000 0.361591 [[-6.95501471  0.21433842  6.7406764 ]\n",
      " [ 0.13668649  0.02956286 -0.16624542]\n",
      " [ 1.2420913   0.1036661  -1.3457557 ]]\n"
     ]
    }
   ],
   "source": [
    "# Corss entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))\n",
    "\n",
    "#Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "    if step % 200 == 0:\n",
    "        print (step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어느정도 학습이 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
